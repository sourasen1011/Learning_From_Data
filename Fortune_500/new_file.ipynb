{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "path = os.path.join(base_path , 'Fortune 500 2017 - Fortune 500.csv')\n",
    "frt_500 = pd.read_csv(path)\n",
    "# Rank seems to be pretty useless - it is based on Revenue \n",
    "frt_500 = frt_500.drop('Rank' , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engg_chain(data,target,feature_sc):\n",
    "    '''\n",
    "    perform feature engg and return df of independent vars and series of dependent vars\n",
    "    params: data -> input dataframe w/o target var\n",
    "    params: target -> target var/ dependent var\n",
    "    params: feature_sc -> sklearn scaler object\n",
    "    returns: data_sd -> dataframe containing standardized numerical values and unstandardized categorical values (encoded as numbers) and standardized dependent variable \n",
    "    '''\n",
    "\n",
    "    # Some transformations\n",
    "    # 1. HQZIP shouldn't be int64 - it is a categorical variable\n",
    "    data['Hqzip'] = data['Hqzip'].astype('str').str.zfill(5)\n",
    "    # 2. Prftchange shouldn't be an object - it is a float. However it needs to be preprocessed - there are some commas etc\n",
    "    try: data['Prftchange'] = data['Prftchange'].apply(lambda x : x.replace(',',''))\n",
    "    except AttributeError: pass\n",
    "    data['Prftchange'] = data['Prftchange'].astype('float64')\n",
    "\n",
    "    # Does Geography play a role?\n",
    "    # Can we encode zip into lat-long info?\n",
    "    nomi = pgeocode.Nominatim('us')\n",
    "    data['lat'] = data['Hqzip'].apply(lambda x: nomi.query_postal_code(x).latitude)\n",
    "    data['long'] = data['Hqzip'].apply(lambda x: nomi.query_postal_code(x).longitude) \n",
    "\n",
    "    num_cov = list(data.describe().columns)\n",
    "    cat_cov = list(data.describe(include = 'object').columns)\n",
    "\n",
    "    num_data = data.loc[: , num_cov]\n",
    "    cat_data = data.loc[: , cat_cov]\n",
    "\n",
    "    # cat_data also has some info in it that may be useful in predicting rev / clustering. Let's label encode them.\n",
    "    le_1 = LabelEncoder()\n",
    "    le_2 = LabelEncoder()\n",
    "\n",
    "    l_encoded_sector = le_1.fit_transform(cat_data.Sector)\n",
    "    l_encoded_industry = le_2.fit_transform(cat_data.Industry)\n",
    "\n",
    "    # Dependent vs Independent vars\n",
    "    # X_num = num_data.loc[: , num_data.columns != 'Revenues']\n",
    "    # y = target.loc[: , 'Revenues']\n",
    "\n",
    "    # Remove multicollinearity\n",
    "    num_data['Profits_per_asset'] = num_data['Profits']/num_data['Assets'] # profit per asset\n",
    "    num_data['Totshequity_per_asset'] = num_data['Totshequity']/num_data['Assets'] # Totshequity per asset\n",
    "    num_data.drop(['Profits' , 'Totshequity'] , axis = 1 , inplace = True)\n",
    "    \n",
    "    # Scaling independent vars\n",
    "    num_data_sd = pd.DataFrame(feature_sc.fit_transform(num_data) , columns = num_data.columns)\n",
    "\n",
    "    X_sd = pd.concat([\n",
    "        num_data_sd,\n",
    "        pd.Series(l_encoded_sector , name = 'sector') , \n",
    "        pd.Series(l_encoded_industry , name = 'industry')\n",
    "    ], axis=1)\n",
    "    \n",
    "    target_sd = pd.DataFrame(np.log10(target), columns = [\"Revenues\"])\n",
    "    data_sd = pd.concat([X_sd , target_sd], axis = 1)\n",
    "    return data_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sc = StandardScaler()\n",
    "data_sd = feature_engg_chain(frt_500[[col for col in frt_500 if col!=\"Revenues\"]], frt_500.Revenues, feature_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4 , figsize = (20,3))\n",
    "\n",
    "ax[0].boxplot(frt_500.Revenues);\n",
    "ax[0].set_xticklabels(['Revenues']);\n",
    "ax[0].set_ylabel('Revenues');\n",
    "\n",
    "ax[1].hist(frt_500.Revenues , bins = 50)\n",
    "ax[1].set_xlabel('Revenues');\n",
    "ax[1].set_ylabel('Number of companies');\n",
    "\n",
    "ax[2].boxplot(np.log10(frt_500.Revenues));\n",
    "ax[2].set_xticklabels(['Revenues']);\n",
    "ax[2].set_ylabel('Log10 Revenues');\n",
    "\n",
    "ax[3].hist(np.log10(frt_500.Revenues) , bins = 50)\n",
    "ax[3].set_xlabel('Log10 Revenues');\n",
    "ax[3].set_ylabel('Number of companies');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "pca = PCA(n_components=n)\n",
    "prin_comp = pca.fit_transform(data_sd)\n",
    "prin_comp_df = pd.DataFrame(prin_comp , columns = ['PC'+str(comp) for comp in range(1,n+1)])\n",
    "print(prin_comp.shape , pca.explained_variance_ratio_ , sum(pca.explained_variance_ratio_)) # about 98% var is explained. great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = LinearSegmentedColormap.from_list('custom blue', [(0,'#ffc400'),(1,'#ff0000')], N=100)\n",
    "\n",
    "fig , ax = plt.subplots()\n",
    "\n",
    "ax1 = ax.scatter(prin_comp_df.PC1 , prin_comp_df.PC2 , c = data_sd['Revenues'] , s = 5, cmap=cmap)\n",
    "ax.grid(alpha = 0.8);\n",
    "# ax.set_xscale('log');\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "\n",
    "# ax.set_yscale('log');\n",
    "ax.set_ylabel('Principal Component 2');\n",
    "\n",
    "cbar = fig.colorbar(ax1)\n",
    "cbar.set_label('Log10 Revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Features before clustering\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(prin_comp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors = 5 as kneighbors function returns distance of point to itself (i.e. first column will be zeros) \n",
    "n = 10\n",
    "neighbours = NearestNeighbors(n_neighbors=n)\n",
    "neighbours_fit = neighbours.fit(scaled_features)\n",
    "# Find the k-neighbors of a point\n",
    "neighbours_dist, _ = neighbours.kneighbors(scaled_features)\n",
    "\n",
    "k_dist = np.sort(neighbours_dist[: , n-1])\n",
    "\n",
    "plt.plot(k_dist)\n",
    "plt.axhline(y=0.26, linewidth=1, linestyle='dashed', color='k')\n",
    "plt.ylabel(\"k-NN distance\")\n",
    "plt.xlabel(f\"Sorted observations ({n-1}th NN)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.26, min_samples=10).fit(scaled_features)\n",
    "# get cluster population\n",
    "counted = Counter(clusters.labels_)\n",
    "counted.get(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(8,4),'legend.fontsize':8.5})\n",
    "p = sns.scatterplot(data=pd.DataFrame(scaled_features ,columns = [\"PC1\",\"PC2\"]), x=\"PC1\", y=\"PC2\", hue=clusters.labels_, legend=\"full\", palette=\"deep\")\n",
    "sns.move_legend(p, \"upper right\", bbox_to_anchor=(1.17, 1), title='Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search through DBSCAN params\n",
    "min_samples = [5,10,20,25,30,50]\n",
    "eps = [0.01,0.05,0.1,0.25,0.4]\n",
    "\n",
    "outliers = {}\n",
    "\n",
    "import itertools\n",
    "for samples , eps in itertools.product(min_samples,eps):\n",
    "    clusters = DBSCAN(eps=eps, min_samples=samples).fit(scaled_features)\n",
    "    # get cluster population\n",
    "    counted = Counter(clusters.labels_)\n",
    "    outliers[(samples,eps)] = counted.get(-1)\n",
    "\n",
    "# Combination with least outliers\n",
    "print(combo := min(outliers , key = outliers.get))\n",
    "samples , eps = combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=eps, min_samples=samples).fit(scaled_features)\n",
    "# get cluster population\n",
    "counted = Counter(clusters.labels_)\n",
    "\n",
    "sns.set(rc={'figure.figsize':(8,4),'legend.fontsize':11.5})\n",
    "p = sns.scatterplot(data=pd.DataFrame(scaled_features ,columns = [\"PC1\",\"PC2\"]), x=\"PC1\", y=\"PC2\", hue=clusters.labels_, legend=\"full\", palette=\"deep\")\n",
    "sns.move_legend(p, \"upper right\", bbox_to_anchor=(1.17, 1), title='Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_kwargs = {\n",
    "\"init\": \"random\",\n",
    "\"n_init\": 10,\n",
    "\"max_iter\": 300,\n",
    "\"random_state\": SEED,\n",
    "}\n",
    "\n",
    "# A list holds the SSE values for each k\n",
    "sse = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(scaled_features)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), sse)\n",
    "plt.xticks(range(1, 11))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.vlines(5, ymin = 0, ymax = 2000 , ls = '--' , color = 'gray');\n",
    "plt.ylim(0,2000);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "init=\"random\",\n",
    "n_clusters=5,\n",
    "n_init=10,\n",
    "max_iter=300,\n",
    "random_state=SEED\n",
    ")\n",
    "\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "\n",
    "prin_comp_df['cluster'] = labels\n",
    "print(prin_comp_df.groupby('cluster').agg({'cluster':'count'}))\n",
    "\n",
    "fig , ax = plt.subplots()\n",
    "\n",
    "for cluster in prin_comp_df.cluster.unique():\n",
    "    df = prin_comp_df[\n",
    "        prin_comp_df['cluster'] == cluster\n",
    "    ]\n",
    "\n",
    "    ax.scatter(df.PC1 , df.PC2 , label = f'cluster no.:-> {cluster}' , s = 10)\n",
    "    ax.legend(loc = 'best')\n",
    "    ax.grid(alpha = 0.8);\n",
    "    # ax.set_xscale('log');\n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "\n",
    "    # ax.set_yscale('log');\n",
    "    ax.set_ylabel('Principal Component 2');\n",
    "\n",
    "plt.legend(loc = 'best', bbox_to_anchor = (1,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Transforms\n",
    "num_cov = ['Employees','Revchange','Prftchange','Assets','lat','long','Profits_per_asset','Totshequity_per_asset']\n",
    "num_data = pd.DataFrame(feature_sc.inverse_transform(data_sd[num_cov]) , columns = num_cov)\n",
    "\n",
    "y = 10**(data_sd.Revenues)\n",
    "\n",
    "sector_industry = data_sd[['sector','industry']]\n",
    "\n",
    "data_clustered = pd.concat(\n",
    "    [\n",
    "        frt_500.Title,\n",
    "        frt_500.Sector,\n",
    "        num_data,\n",
    "        sector_industry,\n",
    "        pd.Series(labels, name='cluster'),\n",
    "        y\n",
    "    ],\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clustered.groupby(\n",
    "    [\n",
    "        'cluster',\n",
    "        'Sector'\n",
    "        ]\n",
    "    ).agg({'Title':'count'}).reset_index().pivot_table(\n",
    "    index = 'Sector' , columns = 'cluster').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Employees', 'Revchange', 'Prftchange', 'Assets', 'Profits_per_asset', 'Totshequity_per_asset', 'Revenues']\n",
    "\n",
    "fig, ax = plt.subplots(len(numeric_cols), data_clustered.cluster.nunique() , figsize = (20,20))\n",
    "\n",
    "for row_idx, row in enumerate(numeric_cols):\n",
    "    for cluster in data_clustered.cluster.unique():\n",
    "        ax[row_idx][cluster].hist(data_clustered[data_clustered['cluster'] == cluster][row] , bins = 20, density=True)\n",
    "        ax[row_idx][cluster].set_xlabel(f'Cluster {cluster}')\n",
    "        ax[row_idx][cluster].set_ylabel(f'{row}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = frt_500[[col for col in frt_500.columns if col!=\"Revenues\"]]\n",
    "y = frt_500[\"Revenues\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frt_500[[col for col in frt_500 if col!=\"Revenues\"]].shape)\n",
    "frt_500[[col for col in frt_500 if col!=\"Revenues\"]].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaler = StandardScaler()\n",
    "feature_engg_chain(X_train, y_train, train_scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "719571bc65a74e1d688dce095ec38766212055f912c80ea1932d85d313f79f3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
